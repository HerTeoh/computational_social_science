{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping for pokemon base stat data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Attack Defense  HP Sp. Atk Sp. Def Speed\n",
      "Raticate       81      60  55      50      70    97\n",
      "Venusaur       82      83  80     100     100    80\n",
      "Charmeleon     64      58  58      80      65    80\n",
      "Blastoise      83     100  79      85     105    78\n",
      "Beedrill       90      40  65      45      80    75\n",
      "Rattata        56      35  30      25      35    72\n",
      "Pidgeotto      60      55  63      50      50    71\n",
      "Butterfree     45      50  60      90      80    70\n",
      "Charmander     52      43  39      60      50    65\n",
      "Ivysaur        62      63  60      80      80    60\n",
      "Wartortle      63      80  59      65      80    58\n",
      "Pidgey         45      40  40      35      35    56\n",
      "Weedle         35      30  40      20      20    50\n",
      "Caterpie       30      35  45      20      20    45\n",
      "Bulbasaur      49      49  45      65      65    45\n",
      "Squirtle       48      65  44      50      64    43\n",
      "Kakuna         25      50  45      25      25    35\n",
      "Metapod        20      55  50      25      25    30\n",
      "Pidgeot        80      75  83      70      70   101\n",
      "Charizard      84      78  78     109      85   100\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "imports relevant libraries\n",
    "'''\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "'''\n",
    "Reads in the html from the url \n",
    "'''\n",
    "url = \"https://pokemondb.net/pokedex/game/lets-go-pikachu-eevee\"\n",
    "headers = {'user-agent':'Mozilla/5.0'}\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "html = urllib.request.urlopen(request).read()\n",
    "\n",
    "'''\n",
    "Pass the HTML into Beautifulsoup and find the table of all pokemon\n",
    "'''\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "main_table = soup.find(\"div\",attrs={'class':\"infocard-list infocard-list-pkmn-lg\"})\n",
    "pokemon = main_table.find_all(\"a\",class_=\"ent-name\")\n",
    "\n",
    "# to collect just the first 20 pokemon stats\n",
    "pokemon = pokemon[:20]\n",
    "\n",
    "'''\n",
    "Define a function to collect the six stats for each pokemon\n",
    "'''\n",
    "def stat_collect(url):\n",
    "    headers = {'user-agent':'Mozilla/5.0'}\n",
    "    request = urllib.request.Request(url,headers=headers)\n",
    "    html = urllib.request.urlopen(request).read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    stats_table = soup.find(attrs={'class':'grid-col span-md-12 span-lg-8'})\n",
    "    stat_name = np.array([i.contents for i in stats_table.find_all('th')][:6]).flatten()\n",
    "    # Because of a lack of ids and repeated class names, this section has to\n",
    "    # jump through and collect every third value to collect only the base stats\n",
    "    temp_array = np.array([j.contents for j in stats_table.find_all('td', attrs={'class':'cell-num'})]).flatten()\n",
    "    stat_value = []\n",
    "    for ind, num in enumerate(temp_array):\n",
    "        if ind%3 == 0:\n",
    "            stat_value.append(num)\n",
    "    return(list(zip(stat_name, stat_value)))\n",
    "\n",
    "'''\n",
    "Cycle through each URL and use the function to stitch pokemon name and stat together\n",
    "'''\n",
    "pokemon_names = np.array([j.contents for j in pokemon]).flatten()\n",
    "pokemon_data = []\n",
    "for link in pokemon:\n",
    "    url = link['href']\n",
    "    if not url.startswith('http'):\n",
    "        url = \"https://pokemondb.net\"+ url\n",
    "    pokemon_data.append(dict(stat_collect(url)))\n",
    "\n",
    "'''\n",
    "Compiles into a dictionary and then into a dataframe\n",
    "'''\n",
    "name_and_stats = dict(zip(pokemon_names,pokemon_data))\n",
    "\n",
    "pokemon_df = pd.DataFrame(name_and_stats).T\n",
    "pokemon_df = pokemon_df.sort_values(by=['Speed'], ascending=False)\n",
    "print(pokemon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            attack  defense  hp  special-attack  special-defense  speed\n",
      "pidgeot         80       75  83              70               70    101\n",
      "charizard       84       78  78             109               85    100\n",
      "raticate        81       60  55              50               70     97\n",
      "venusaur        82       83  80             100              100     80\n",
      "charmeleon      64       58  58              80               65     80\n",
      "blastoise       83      100  79              85              105     78\n",
      "beedrill        90       40  65              45               80     75\n",
      "rattata         56       35  30              25               35     72\n",
      "pidgeotto       60       55  63              50               50     71\n",
      "butterfree      45       50  60              90               80     70\n",
      "charmander      52       43  39              60               50     65\n",
      "ivysaur         62       63  60              80               80     60\n",
      "wartortle       63       80  59              65               80     58\n",
      "pidgey          45       40  40              35               35     56\n",
      "weedle          35       30  40              20               20     50\n",
      "caterpie        30       35  45              20               20     45\n",
      "bulbasaur       49       49  45              65               65     45\n",
      "squirtle        48       65  44              50               64     43\n",
      "kakuna          25       50  45              25               25     35\n",
      "metapod         20       55  50              25               25     30\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "imports relevant libraries\n",
    "'''\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Requests for a list of pokemon names\n",
    "'''\n",
    "response = requests.get(\"https://pokeapi.co/api/v2/pokemon/\")\n",
    "data = response.json()\n",
    "pokemon = pd.DataFrame(data[\"results\"])\n",
    "pokemon_names = pokemon['name'].tolist()\n",
    "\n",
    "'''\n",
    "Goes through each name and collects the base stats for it\n",
    "'''\n",
    "name_and_stats = {}\n",
    "for name in pokemon_names:\n",
    "    pokemon_data = requests.get(\"https://pokeapi.co/api/v2/pokemon/%s\"%name)\n",
    "    pokemon_data = pokemon_data.json()\n",
    "    stats = dict([(i['stat']['name'],i['base_stat']) for i in pokemon_data['stats']])\n",
    "    name_and_stats[name] = stats\n",
    "\n",
    "'''\n",
    "Compiles into a dataframe\n",
    "'''\n",
    "name_and_stats = pd.DataFrame(name_and_stats).T\n",
    "pokemon_df = name_and_stats.sort_values(by=['speed'], ascending=False)\n",
    "print(pokemon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a quick little project to explore web scraping using Beautiful Soup. Note that all the code above took multiple iterations. Through it I refreshed my memory on reading HTML, converting json files and parsing incoming data.\n",
    "\n",
    "Collecting the data through the API was much easier. The API was well documented and specific data could be requested without much cleaning or parsing. Above, it took half the number of lines of code to collect the same data using the API compared to the web scraper.\n",
    "\n",
    "But that isn't to say that the web scraper is inferior. An API is not always avaialble and a web scraper is useful because it is enables a user to collect information from what they can see on the web. What you can see on a webpage, you can likely collect.\n",
    "\n",
    "In this instance, it might be useful to be able to use the API and web scraper to cross reference pokemon and pokemon stats from both sources in order to identify any pieces of missing data or discrepancies. \n",
    "\n",
    "\\* this was a small project that did not overload the servers, but to prevent being blacklisted and maintaining appropriate web etiqquette, it is important to read the terms and conditions, check for a robot.txt file, not overload the servers and input some randomness into the scraper to avoid detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
